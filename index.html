<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Yu Lab’s Pages</title>

    <!-- Bootstrap -->
    <link href="bootstrap.min.css" rel="stylesheet">
    <script src="http://code.jquery.com/jquery-1.11.0.min.js"></script>
    <script src="js/bootstrap.min.js"></script>


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  
    <h1 class="project-name">Jingjing Yu</h1>
    
  </header>

     
  <body>
 <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
            <!-- Brand and toggle get grouped for better mobile display -->
        <div class="container">
          <div class="navbar-header" >
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="#" style="color:#159957 ">Home</a>
                <a class="navbar-brand" href= "#research" style="color:#159957" >Research</a>
                <!-- <a class="navbar-brand" href="#paper" style="color:#159957" >Paper</a> -->
                <a class="navbar-brand" href="#group" style="color:#159957" >Group</a>
                <a class="navbar-brand" href="./prospective.html" style="color:#159957">Contact Us</a>
        
            </div>
            <!-- /.-collapse -->
        </div>
        <!-- /.container -->
    </nav>

      
    <div class="container">
        <div class="bs-docs-section">  
            <div class="page-header" style="margin-top: 50px;">
                <div class="row" style="padding-bottom: 0px;">
                <!-- <div class="col-lg-12">
                    <div class="col-xs-7 col-md-8" style="text-align:left;"><img src="images/name_tmp.png" height = "60" style="max-height: 100%; max-width: 100%;"/></div>
                    <div class="col-xs-5 col-md-4" style="text-align:right;"><img src="images/name.png" height = "60" style="max-height: 100%; max-width: 100%;"/></div>
                </div> -->
                </div>
            </div>
        

        <a name="bio" style="visibility: hidden;"></a>
        <div class="row">
        <div class="col-lg-12">
            <!-- <div class="col-xs-4 col-md-2" style="text-align: center; vertical-align: middle;">
                <img src="images/self-photo.jpg" style="max-height: 95%; max-width: 95%;" />
            </div> -->
            <div class="col-md-7" style="vertical-align: middle;">       
                <p>
                    Jingjing Yu is a professor at <a href="https://www.bjtu.edu.cn/">Beijing Jiaotong University</a> school of <a href="http://eie.bjtu.edu.cn/">Electronic and Information Engineering</a>.
                </p>
                <p>
                    Her research interests include array signal processing, multidimensional signal processing and intelligent cooperative sensing, and multimedia signal processing.
                    This includes microphone array signal processing, multidimensional information processing and cooperative optimization of intelligent sensor networks based on big data analysis, 
                    three-dimensional immersive virtual environment and visual reconstruction, theories and methods of network user privacy protection and data security sharing based on big data correlation analysis, etc. 
                    <!-- assistant professor at  <a href="https://ucsd.edu/"> UC San Diego </a> department of <a href="https://cse.ucsd.edu/"> Computer Science and Engineering</a>. 
                    She is a primary faculty with the <a href="http://ai.ucsd.edu"> AI Group</a> and is affiliated with  <a href="https://datascience.ucsd.edu/">Halıcıoğlu Data Science Institute</a>.  -->
                </p>

                <p>
<!--                    Before that, I received my Ph.D. degree in <a href="http://www.cs.usc.edu/">Computer Science </a> from <a href="http://www.usc.edu/">University of Southern California</a>.-->
<!-- Her research interests lie primarily in <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>, especially for large-scale spatiotemporal data. She is particularly excited about AI for scientific discovery. 
She has won <a href="https://cse.ucsd.edu/about/news/rose-yu-receives-army-early-career-program-award">ECASE Award</a>, <a href="https://cse.ucsd.edu/about/news/trifecta-cse-faculty-are-honored-nsf-career-awards">NSF CAREER Award</a>, <a href="https://evc.ucsd.edu/resources/hellman-fellowship-recipients.html#2022-23-Fellows">Hellman Fellowship</a>, Faculty Awards from JP Morgan, Meta, Google, Amazon, and Adobe, several Best Paper Awards, and Best Dissertation Award at USC.  -->

<p>For more details, see <a href="http://eie.bjtu.edu.cn/WebHtml/szdw/0406">Curriculum Vitae</a>.</p>
                </p>

            </div>
            <div class="col-md-3">
                <h4 style="margin-top: auto;">Contact Info</h4>
                <ul>
                    <li>E-mail: jjyu@bjtu.edu.cn</li>
                    <li> Phone:86-10-51688276</li>
                    <!-- <li> Office: CSE 4216 </li> -->
                    <li>Address: 111111111111111111111</li>
                </ul>
               
            </div>
        </div>
        </div>


<!-- #####################################################  Research   ######################################-->
        <!-- <div class="bs-docs-section">
            <div class="row">
              <div class="col-lg-12">
                <div class="page-header">
                   <h2 id="research">Research</h2>
                </div>
               </div>
            </div>

            <div class="row">
              <div class="col-md-10">
                  <h3>Main research topics</h3>
                  <h4>Multichannel Signal Processing</h4>
                  <h4>Microphone Array Signal Processing</h4>
                  <h4>IoT Sensor Network  </h4>
              </div>
            </div>           -->
<!-- #####################################################  Research   ######################################-->
      <div class="bs-docs-section">
        <div class="row">
          <div class="col-lg-12">
              <div class="page-header">
                  <h2 id="research">Research</h2>
              </div>
            </div>
        </div>
            

            <!-- /.row -->
          <div class="row">

            <!-- <div class="col-md-10">
              <h3>Submitted</h3>
            </div>               -->
  <!---------------------------- paper1 ------------------------------->

  <!-- #########################       Multichannel Signal Processing         ############################ -->
            <div class="col-md-10">  
                <h3> <b>Multichannel Signal Processing </b></h3>
                <p>Need to be updated</p>
                  <ul> 
                    <li id="icml2023-A">
                      <h4><b>Self-Supervised Speech Enhancement Based on Speaker Verification</b></h4>
                      <b>L. Zhang, J. Yu , C. yang, L.Wu</b></br>
                      <b>Submitted to <em> IEEE International Conference on Acoustics, Speech and Signal Processing  </em> (ICASSP) , 2024</b></br>
                      This paper proposes a self-supervised speech enhancement algorithm based on speaker verification, aiming to achieve robust 
                      speech enhancement of target speaker in the absence of ground-truth data and unseen scene conditions. 
                      Speaker verification posterior probability based on few-shot samples of target speaker is computed and acts as pseudo label 
                      to extract related speaker features. The feature-aware framework combines attention-based noise prediction and target semantic feature extraction, 
                      to improve the robustness of the network. Self-supervised adversarial training is applied with Minimax manners 
                      to extract high-quality target speaker speech, while suppressing background noise and interferences.
                      [<a href="https://ieeexplore.ieee.org/document/9746054">Paper</a>]
                      [<a href="https://yulabs2.github.io/">Audio clips</a>]
                    </li>
                    <li id="iclr2021-A">
                      <h4><b>Self-Supervised Generative Network for Personalized Speech Enhancement</b></h4>
                      <b>C. yang, J. Yu , L. Zhang, L.Wu</b></br>
                      <b>Submitted to <em> IEEE International Conference on Acoustics, Speech and Signal Processing  </em>(ICASSP) , 2024 </b></br>
                      This paper proposes a self-supervised speech enhancement algorithm based on speaker 
                      verification, aiming to achieve robust speech enhancement of target speaker in the absence of ground-truth data and unseen 
                      scene conditions. Speaker verification posterior probability based on few-shot samples of target speaker is computed and 
                      acts as pseudo label to extract related speaker features. The feature-aware framework combines attention-based noise 
                      prediction and target semantic feature extraction, to improve the robustness of the network. Self-supervised adversarial 
                      training is applied with Minimax manners to extract high-quality target speaker speech, while suppressing background 
                      noise and interferences.   
                      [<a href="">Paper</a>]
                      [<a href="https://yulabs2.github.io/">Audio clips</a>] 
                      </li>
                  </ul> 
            </div>
            <div class="col-md-2">
                <a href="#">
                    <img class="img-responsive" src="images/fig1.png" alt="", height="400" width="400">
                </a>
            </div>
  <!-- #####################################   IOT  ###################################### -->
            <div class="col-md-10">  
              <h3> <b>Multichannel Signal Processing </b></h3>
              <p>wuwuwuwuwwwwwwwwwwwwwwwwwwww </p>
                <ul> 
                  <li id="icml2023-A">
                    <h4><b>Self-Supervised Speech Enhancement Based on Speaker Verification</b></h4>
                    <b>L. Zhang, J. Yu , C. yang, L.Wu</b></br>
                    <b>Submitted to <em> IEEE International Conference on Acoustics, Speech and Signal Processing  </em> (ICASSP) , 2024</b></br>
                    This paper proposes a self-supervised speech enhancement algorithm based on speaker verification, aiming to achieve robust 
                    speech enhancement of target speaker in the absence of ground-truth data and unseen scene conditions. 
                    Speaker verification posterior probability based on few-shot samples of target speaker is computed and acts as pseudo label 
                    to extract related speaker features. The feature-aware framework combines attention-based noise prediction and target semantic feature extraction, 
                    to improve the robustness of the network. Self-supervised adversarial training is applied with Minimax manners 
                    to extract high-quality target speaker speech, while suppressing background noise and interferences.
                    [<a href="https://ieeexplore.ieee.org/document/9746054">Paper</a>]
                    [<a href="https://yulabs2.github.io/">Audio clips</a>]
                  </li>
                  <li id="iclr2021-A">
                    <h4><b>Self-Supervised Generative Network for Personalized Speech Enhancement</b></h4>
                    <h5>C. yang, J. Yu , L. Zhang, L.Wu</h5>
                    <h5>Submitted to <em> IEEE International Conference on Acoustics, Speech and Signal Processing  </em>(ICASSP) , 2024 </h5>
                    This paper proposes a self-supervised speech enhancement algorithm based on speaker 
                    verification, aiming to achieve robust speech enhancement of target speaker in the absence of ground-truth data and unseen 
                    scene conditions. Speaker verification posterior probability based on few-shot samples of target speaker is computed and 
                    acts as pseudo label to extract related speaker features. The feature-aware framework combines attention-based noise 
                    prediction and target semantic feature extraction, to improve the robustness of the network. Self-supervised adversarial 
                    training is applied with Minimax manners to extract high-quality target speaker speech, while suppressing background 
                    noise and interferences.   
                    [<a href="">Paper</a>]
                    [<a href="https://yulabs2.github.io/">Audio clips</a>] 
                    </li>
                </ul> 
          </div>
          <div class="col-md-2">
              <a href="#">
                  <img class="img-responsive" src="images/fig1.png" alt="", height="400" width="400">
              </a>
          </div>
  <!---------------------------- paper2 ------------------------------->
<!--             
            <div class="col-md-10">  
              <h4> IoT Sensor Network</h4>
              <h5> C. yang, J. Yu , L. Zhang, L.Wu </h5>
              <h5> <em>IEEE International Conference on Acoustics, Speech and Signal Processing </em>(ICASSP) , 2024 </br>  </h5>
              <p>This paper proposed a self-supervised generative adversarial framework based on 
                speaker verification posterior probability. Considering the 
                sparsity of sounds, three-dimensional mask is applied with 
                the generator to reconstruct signals from multiple sources, 
                without requiring the clean speech of the target. A speaker 
                verification model is applied as the discriminator to identify 
                features related to the target speaker based on only few-shot 
                samples. Adversarial training is performed with speaker 
                verification posterior probability, reconstruction loss and 
                channel-independent loss for personalized speech 
                enhancement.</p>
                <ul> 
                  <li id="icml2023-A">
                    <b>Generative Adversarial Symmetry Discovery</b></br> 
                    Jianke Yang, Robin Walters, Nima Dehmamy, Rose Yu</br>
                    <em>International Conference on Machine Learning</em> (ICML), 2023 </br>
                    [<a href="https://ieeexplore.ieee.org/document/9746054">Paper</a>]
                    [<a href="https://yulabs2.github.io/">Audio clips</a>]
                  </li>
                  <li id="iclr2021-A">
                    <b> Incorporating Symmetry into Deep Dynamics Models for Improved Generalization </b></br>
                    Rui Wang*, Robin Walters*, Rose Yu </br>  
                    <em>International Conference on Learning Representations</em> (ICLR), 2021 </br>
                    [<a href="https://ieeexplore.ieee.org/document/9746054">Paper</a>]
                    [<a href="https://yulabs2.github.io/">Audio clips</a>] 
                    </li>
                </ul> 
            </div>
            <div class="col-md-2">
                <a href="#">
                    <img class="img-responsive" src="images/fig1.png" alt="", height="400" width="400">
                </a>
            </div> -->
  <!---------------------------- paper2 ------------------------------->
            <!-- <div class="col-md-10">
              
              <h4> Dynamic Multimensional  Spacial-Temporal Graph Convalutional Network for Scene-Adaptive Speech Enhancement</h4>
              <h5> L. Zhang, J. Yu ,M. Hao, C.Yang   </h5>
              <h5> <em>IEEE International Conference on Acoustics, Speech and Signal Processing </em>(ICASSP) , 2023 </br>  </h5>
              <p>This paper proposes a dynamic multidimensional spatial-temporal graph convolutional network for multichannel speech enhancement, 
                aiming at improving the robustness of the network in various scenarios.
                Multidimensional graph convolutional block to adaptively capture the time-varying inter-channel spatial correlations is embedded with the time-frequency convolution block 
                to fuse the multidimensional correlation features for target speech estimation.
                Scene-adaptive hypermatrix including flexible multidimensional graph operator, adaptive adjacency matrix and dynamic time-frequency correlation matrix is proposed 
                to enhance the generalization and robustness of network over different noisy scenes. </p>
              <ul> 
                <li id="icml2023-A">
                  <b>Generative Adversarial Symmetry Discovery</b></br> 
                  Jianke Yang, Robin Walters, Nima Dehmamy, Rose Yu</br>
                  <em>International Conference on Machine Learning</em> (ICML), 2023 </br>
                  [<a href="https://ieeexplore.ieee.org/document/9746054">Paper</a>]
                  [<a href="https://yulabs2.github.io/">Audio clips</a>]
                </li>
                <li id="iclr2021-A">
                  <b> Incorporating Symmetry into Deep Dynamics Models for Improved Generalization </b></br>
                  Rui Wang*, Robin Walters*, Rose Yu </br>  
                  <em>International Conference on Learning Representations</em> (ICLR), 2021 </br>
                  [<a href="https://ieeexplore.ieee.org/document/9746054">Paper</a>]
                  [<a href="https://yulabs2.github.io/">Audio clips</a>] 
                  </li>
              </ul> 
            </div>
            <div class="col-md-2">
                <a href="#">
                    <img class="img-responsive" src="images/fig3.png" alt="", height="250" width="250">
                </a>

            </div>         
          </div> -->
    <!---------------------------- published paper ------------------------------->
            <!-- <div class="row">
              <div class="col-md-10">
                <h3>Pubulished</h3>
              </div>
              <div class="col-md-10">
                  <h4> Spatial-Temporal Graph Convolution Network for Multichannel Speech Enhancement</h4>
                  <h5> M. Hao, J. Yu , L. Zhang  </h5>
                  <h5> <em>IEEE International Conference on Acoustics, Speech and Signal Processing </em>(ICASSP) , 2022 </br>  </h5>
                  <p>This paper proposes a spatial-temporal graph convolutional network composed of cascaded
                    spatial-temporal (ST) modules with channel fusion. Without 
                    any prior information of array and acoustic scene, a graph 
                    convolution block is designed with learnable adjacency 
                    matrix to capture the spatial dependency of pairwise channels. 
                    Then, it is embedded with time-frequency convolution block 
                    as the ST module to fuse the multi-dimensional correlation
                    features for target speech estimation. Furthermore, a novel 
                    weighted loss function based on speech intelligibility index 
                    (SII) is proposed to assign more attention for the important 
                    bands of human understanding during network training.   </p>
                    [<a href="https://ieeexplore.ieee.org/document/9746054">Paper</a>]
                    [<a href="https://ahuei.github.io/stgcsen/">Audio clips</a>]
              </div>
              <div class="col-md-2">
                  <a href="#">
                      <img class="img-responsive" src="images/Hao.png" alt="", height="250" width="250">
                  </a>
              </div> -->


               <!-- <div class="col-md-10">
               <a href="#point" data-toggle="collapse">Read More</a>
                  <div id="point" class="collapse">
                   <ul>  
                    
                    <li id="l4dc2023-B">
                      <b>Spatial-Temporal Graph Convolution Network for Multichannel Speech Enhancement</b></br>
                      M. Hao, J. Yu , L. Zhang</br>  
                      <em>IEEE International Conference on Acoustics, Speech and Signal Processing </em>(ICASSP) , 2022 </br>  
                    </li>

                   </div>
                   </ul> 
               </div> -->
            </div>
      </div>
        
        <!-- /.section --> 

<div class="bs-docs-section">
  <div class="row">
    <div class="col-lg-12">
      <div class="page-header">
         <h2 id="group">Group</h2>
      </div>
     


    <h3>Current</h3>  
    <h5>PhD Students & Postdocs </h5>  
    <ul> 
     <li id="current"> 
      <a href="https://salvarc.github.io/"> Salva Rühling Cachay </a>
    </li>
    <!-- <li id="current"> 
      <a href="http://people.cs.uchicago.edu/~hytruongson/"> Truong Son Hy </a>
    </li> -->
    <li id="current"> 
        <a href="https://huiwenn.github.io">  Sophia Sun  </a>
     </li>
     <li id="current">
      <a href="https://vsumanth99.github.io/">Sumanth Varambally</a>
    </li>
      <li id="current">
        <a href="https://dongxiaw.github.io/online-cv/"> Dongxia (Allen) Wu  </a>
      </li>
      <li id="current">
        <a href="https://jiankeyang.github.io"> Jianke Yang</a>
      </li>
      <li id="current">
        <a href="https://b-zhao.github.io/">Bo  Zhao </a> 
      </li>
      <li id="current"> 
        <a href="https://zzhou.info/">Zihao  Zhou</a>
    </li>
 

  
    </ul>

    <h5>Undergraduates & Master's Students</h5>
    <ul>
      <li id="current">
        <a href="https://petereckmann1.github.io/"> Peter Eckmann</a>   
      </li>

    
  
      </ul>

    <h3>Alumni</h3>
    <h5>PhD Students & Postdocs </h5>  
    <ul>
      <li id="past"> 
        <a href="https://rui1521.github.io/online-cv/">  Rui (Ray) Wang  </a>  -->Postdoc, MIT
      </li>
    <li id="past"> 
      <a href="https://jkozerawski.github.io/"> Jedrzej (Jacob) Kozerawski </a>  --> Research Scientist, Apple
   </li>
   <li id="past"> 
    <a href="https://www.khoury.northeastern.edu/people/robin-walters/">  Robin Walters  </a> --> Assistant Professor, Northeastern University
 </li>
</ul>
    <h5>Undergraduates & Master's Students</h5>
    <ul>
      <li id="past"> 
         Eliza Huang --> AWS for Health
     </li>
      <li id="past"> 
         Kenneth (Theo) Carr --> MIT-WHOI Joint Program 
      </li>
   
      <li id="past">   
        <a href="https://chnsh.me/">Chintan Shah </a> --> PathAI
      </li>
      <li id="current">
        <a href="https://sharanmayank.github.io/"> Mayank Sharan</a>  --> Microsoft Research
      </li>

      <li id="past"> 
        <a href="https://manish1596.github.io/">Manish Singh</a> --> Qualcomm AI Research
      </li>
      <li id="past">
        <a href="https://www.media.mit.edu/people/raechelw/overview/"> Raechel Walker</a> --> MIT-MAS Program
      </li>
      <li id="past"> 
        <a href="https://sites.google.com/view/fanxie/home"> Fan  Xie </a> --> Even Financial
     </li>


      </ul>
  </div>
</div>

</div>

<hr>
  <!-- /.section -->
     
      <footer>
        <div class="row">
          <div class="col-lg-12">
            <p>Code released under the <a href="https://github.com/thomaspark/bootswatch/blob/gh-pages/LICENSE">MIT License</a>.</p>
            <p>Based on <a href="http://getbootstrap.com" rel="nofollow">Bootstrap</a>. Icons from <a href="http://fortawesome.github.io/Font-Awesome/" rel="nofollow">Font Awesome</a>. Web fonts from <a href="http://www.google.com/webfonts" rel="nofollow">Google</a>.</p>

          </div>
        </div>

     </footer>

      </div>
      <!-- /.container -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-42633780-2', 'auto');
      ga('send', 'pageview');

    </script>
  </body>
 
    
    
    
    
    
</html>
